{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "import os,glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/28/91f26bd088ce8e22169032100d4260614fc3da435025ff389ef1d396a433/pip-20.2.4-py2.py3-none-any.whl (1.5MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5MB 8.2MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: pip\n",
      "  Found existing installation: pip 19.2.3\n",
      "    Uninstalling pip-19.2.3:\n",
      "      Successfully uninstalled pip-19.2.3\n",
      "Successfully installed pip-20.2.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.3.1-cp37-cp37m-macosx_10_9_x86_64.whl (165.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 165.1 MB 11.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
      "  Downloading tensorboard-2.4.0-py3-none-any.whl (10.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.6 MB 8.3 MB/s eta 0:00:01    |█████████████████████████▎      | 8.4 MB 8.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.12.0)\n",
      "Collecting h5py<2.11.0,>=2.10.0\n",
      "  Downloading h5py-2.10.0-cp37-cp37m-macosx_10_6_intel.whl (3.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0 MB 15.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.9.2\n",
      "  Downloading protobuf-3.14.0-cp37-cp37m-macosx_10_9_x86_64.whl (1.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0 MB 12.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.11.2)\n",
      "Collecting astunparse==1.6.3\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (1.17.2)\n",
      "Collecting keras-preprocessing<1.2,>=1.1.1\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 4.1 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[K     |████████████████████████████████| 65 kB 7.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Collecting grpcio>=1.8.6\n",
      "  Downloading grpcio-1.33.2-cp37-cp37m-macosx_10_9_x86_64.whl (3.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.4 MB 9.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
      "  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n",
      "\u001b[K     |████████████████████████████████| 459 kB 6.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /opt/anaconda3/lib/python3.7/site-packages (from tensorflow) (0.33.6)\n",
      "Collecting google-pasta>=0.1.8\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 7.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py>=0.7.0\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 17.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.22.0)\n",
      "Collecting google-auth<2,>=1.6.3\n",
      "  Downloading google_auth-1.23.0-py2.py3-none-any.whl (114 kB)\n",
      "\u001b[K     |████████████████████████████████| 114 kB 15.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /opt/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (41.4.0)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Downloading tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779 kB)\n",
      "\u001b[K     |████████████████████████████████| 779 kB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.15 in /opt/anaconda3/lib/python3.7/site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.16.0)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.3.3-py3-none-any.whl (96 kB)\n",
      "\u001b[K     |████████████████████████████████| 96 kB 5.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Downloading google_auth_oauthlib-0.4.2-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2019.9.11)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.24.2)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Collecting cachetools<5.0,>=2.0.0\n",
      "  Downloading cachetools-4.1.1-py3-none-any.whl (10 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "\u001b[K     |████████████████████████████████| 155 kB 11.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
      "  Downloading rsa-4.6-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 16.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/anaconda3/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (0.23)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[K     |████████████████████████████████| 77 kB 6.7 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (0.6.0)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.1.0-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: more-itertools in /opt/anaconda3/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow) (7.2.0)\n",
      "Building wheels for collected packages: termcolor\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4830 sha256=c6bd07040300abadedca02098a12eca53f8ca3bf8b64cb641bcaf1b4bc856e73\n",
      "  Stored in directory: /Users/mohammedhussain/Library/Caches/pip/wheels/3f/e3/ec/8a8336ff196023622fbcb36de0c5a5c218cbb24111d1d4c7f2\n",
      "Successfully built termcolor\n",
      "Installing collected packages: cachetools, pyasn1, pyasn1-modules, rsa, google-auth, tensorboard-plugin-wit, protobuf, markdown, oauthlib, requests-oauthlib, google-auth-oauthlib, grpcio, absl-py, tensorboard, h5py, astunparse, gast, keras-preprocessing, opt-einsum, termcolor, tensorflow-estimator, google-pasta, tensorflow\n",
      "  Attempting uninstall: h5py\n",
      "    Found existing installation: h5py 2.9.0\n",
      "    Uninstalling h5py-2.9.0:\n",
      "      Successfully uninstalled h5py-2.9.0\n",
      "Successfully installed absl-py-0.11.0 astunparse-1.6.3 cachetools-4.1.1 gast-0.3.3 google-auth-1.23.0 google-auth-oauthlib-0.4.2 google-pasta-0.2.0 grpcio-1.33.2 h5py-2.10.0 keras-preprocessing-1.1.2 markdown-3.3.3 oauthlib-3.1.0 opt-einsum-3.3.0 protobuf-3.14.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.0 rsa-4.6 tensorboard-2.4.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.1 tensorflow-estimator-2.3.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: h5py in /opt/anaconda3/lib/python3.7/site-packages (from Keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/anaconda3/lib/python3.7/site-packages (from Keras) (1.17.2)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/anaconda3/lib/python3.7/site-packages (from Keras) (1.3.1)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.7/site-packages (from Keras) (5.1.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.7/site-packages (from h5py->Keras) (1.12.0)\n",
      "Installing collected packages: Keras\n",
      "Successfully installed Keras-2.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers\n",
    "from keras.models import model_from_yaml\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Conv2D,MaxPooling2D,Dropout,Flatten\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator,array_to_img,img_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Processing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   a) Use the \"ImageDataGenerator()\" class from keras.processing.image to build out an instance called \"train_datagen\" with the following parameters: \n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,shear_range = 0.2,zoom_range = 0.2,horizontal_flip = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# b) Then build your training set by using the method \".flow_from_directory()\"\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    directory = '/Users/mohammedhussain/Desktop/UCHICAGO assigments/Machine learning/assignment 7 nov 20/dataset_train',\n",
    "    target_size = (64,64),\n",
    "    batch_size = 32,\n",
    "    class_mode = \"categorical\",\n",
    "    seed = 66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So train_datagen.flow_from_directory has found around 88 images from 4 different classes or categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the count of all images from each category\n",
    "training_set_files = glob.glob(\"dataset_train/*\")\n",
    "count_images = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_train/category 2': 22,\n",
       " 'dataset_train/category 4': 22,\n",
       " 'dataset_train/category 3': 22,\n",
       " 'dataset_train/category 1': 22}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iterating over each image in the trainning image set in ech category\n",
    "for i in training_set_files:\n",
    "    images_by_category = len(os.listdir(i+'/'))\n",
    "    count_images[i] = images_by_category\n",
    "count_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 different categories or classes and each category/class , there are 22 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1031.png',\n",
       " '1025.png',\n",
       " '1024.png',\n",
       " '1030.png',\n",
       " '1032.png',\n",
       " '1033.png',\n",
       " '1023.png',\n",
       " '1020.png',\n",
       " '1021.png',\n",
       " '1052.png',\n",
       " '1051.png',\n",
       " '1050.png',\n",
       " '1040.png',\n",
       " '1041.png',\n",
       " '1043.png',\n",
       " '1042.png',\n",
       " '1010.png',\n",
       " '1011.png',\n",
       " '1013.png',\n",
       " '1012.png',\n",
       " '1000.png',\n",
       " '1014.png']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in training_set_files:\n",
    "    train_images = list(os.listdir(i+'/'))\n",
    "train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 800, 3)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets take a random sample image from the above list\n",
    "sample_image = image.load_img('dataset_train/category 1/1013.png')\n",
    "x_image = img_to_array(sample_image)\n",
    "x_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of each training image observation is (800,800,3). This is a raw image size , not the input image size to the CNN classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the image after processing\n",
    "training_set.target_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If we look at the actual shapes of the image in the training set , we come across very strange images (chinese symbols,\n",
    " skulls, helmet  cross symbols, star wars helmet etc)                                                                                                     )\n",
    "\n",
    "                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(64,64,3) will be our input to the CNN image classifer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Initial Classifier Build: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 31, 31, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 29, 29, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 14, 14, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               1605760   \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,625,668\n",
      "Trainable params: 1,625,668\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(filters = 32,kernel_size =(3,3), input_shape = (64,64,3),activation = \"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Conv2D(filters = 64,kernel_size =(3,3),activation = \"relu\"))\n",
    "classifier.add(MaxPooling2D(pool_size = (2,2)))\n",
    "classifier.add(Flatten()) #     converting the feature maps to 1D feature vectors\n",
    "classifier.add(Dense(units = 128,activation = \"relu\"))\n",
    "classifier.add(Dense(units = 4,activation = \"softmax\"))\n",
    "classifier.compile(optimizer = 'adam',loss ='categorical_crossentropy',metrics = ['accuracy'])\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Runs: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3/3 [==============================] - 1s 177ms/step - loss: 1.7567 - accuracy: 0.2727\n",
      "Epoch 2/3\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 1.2310 - accuracy: 0.5795\n",
      "Epoch 3/3\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 0.7861 - accuracy: 0.7955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd9474cb410>"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(training_set,steps_per_epoch =3,epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "# Saving Model and its weights to a file\n",
    "my_model = classifier.to_yaml()\n",
    "with open(\"My_Model.yaml\",\"w\") as file:\n",
    "    file.write(my_model)\n",
    "classifier.save('My_Model.h5')\n",
    "print(\"Saved model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# Predicting on the test data images using the model we already built\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('My_Model.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data path\n",
    "img_dir = \"dataset_test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over each test image\n",
    "data_path = os.path.join(img_dir, '*g')\n",
    "files = glob.glob(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_test/C033.png\n",
      "dataset_test/1022.png\n",
      "dataset_test/4011.png\n",
      "dataset_test/1053.png\n",
      "dataset_test/6051.png\n",
      "dataset_test/4053.png\n",
      "dataset_test/C014.png\n",
      "dataset_test/6023.png\n"
     ]
    }
   ],
   "source": [
    "# print the files in the dataset_test folder \n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([3]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([0]),\n",
       " array([1]),\n",
       " array([2]),\n",
       " array([1]),\n",
       " array([1])]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a prediction and add to results \n",
    "data = []\n",
    "results = []\n",
    "for f1 in files:\n",
    "    img = image.load_img(f1, target_size = (64, 64))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis = 0)\n",
    "    data.append(img)\n",
    "    result = model.predict(img)\n",
    "    r = np.argmax(result, axis=1)\n",
    "    results.append(r)\n",
    "\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category 1': 0, 'category 2': 1, 'category 3': 2, 'category 4': 3}"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # d) Determine accuracy.\n",
    "\n",
    "# check category labels in training_set\n",
    "\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_test/C033.png\n",
      "dataset_test/1022.png\n",
      "dataset_test/4011.png\n",
      "dataset_test/1053.png\n",
      "dataset_test/6051.png\n",
      "dataset_test/4053.png\n",
      "dataset_test/C014.png\n",
      "dataset_test/6023.png\n"
     ]
    }
   ],
   "source": [
    " # First we need to understand the sequence of how the images were read in the test dataset directory\n",
    "for f1 in files :\n",
    "    print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual labelling\n",
    "test_labels= [3, 0, 2, 0, 1, 2, 3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating accuracy by comparing the predicted values to the actual values for the test set and calculate accuracy score\n",
    "accuracy = metrics.accuracy_score(test_labels,results)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Run this process for the following combinations:\n",
    "\n",
    "* (steps_per_epoch: 1, epochs: 1)\n",
    "* (steps_per_epoch: 1, epochs: 2)\n",
    "* (steps_per_epoch: 1, epochs: 3)\n",
    "* (steps_per_epoch: 2, epochs: 4)\n",
    "* (steps_per_epoch: 2, epochs: 5)\n",
    "* (steps_per_epoch: 2, epochs: 6)\n",
    "* (steps_per_epoch: 3, epochs: 7)\n",
    "* (steps_per_epoch: 3, epochs: 8)\n",
    "* (steps_per_epoch: 5, epochs: 9)\n",
    "* (steps_per_epoch: 5, epochs: 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = [\n",
    " {\"model_name\": \"model_1\", \"steps_per_epoch\": 1, \"epochs\": 1},\n",
    " {\"model_name\": \"model_2\", \"steps_per_epoch\": 1, \"epochs\": 2},\n",
    " {\"model_name\": \"model_3\", \"steps_per_epoch\": 1, \"epochs\": 3},\n",
    " {\"model_name\": \"model_4\", \"steps_per_epoch\": 2, \"epochs\": 4},\n",
    " {\"model_name\": \"model_5\", \"steps_per_epoch\": 2, \"epochs\": 5},\n",
    " {\"model_name\": \"model_6\", \"steps_per_epoch\": 2, \"epochs\": 6},\n",
    " {\"model_name\": \"model_7\", \"steps_per_epoch\": 3, \"epochs\": 7},\n",
    " {\"model_name\": \"model_8\", \"steps_per_epoch\": 3, \"epochs\": 8},\n",
    " {\"model_name\": \"model_9\", \"steps_per_epoch\": 5, \"epochs\": 9},\n",
    " {\"model_name\": \"model_10\", \"steps_per_epoch\": 5,\"epochs\": 10}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the image classifier model to fit and save the weights to the file/disk\n",
    "def model_fit_save(model_params):\n",
    "    print(f\"Fitting the model: {model_params}\\n\\n\")\n",
    "    classifier_fit = classifier.fit(train_datagen.flow_from_directory(\n",
    "    directory = '/Users/mohammedhussain/Desktop/UCHICAGO assigments/Machine learning/assignment 7 nov 20/dataset_train',\n",
    "    target_size = (64,64),\n",
    "    batch_size = 32,\n",
    "    class_mode = \"categorical\"), steps_per_epoch=model_params[\"steps_per_epoch\"], epochs=model_params[\"epochs\"])\n",
    "    # Saving the model and its weights as a file to the disk\n",
    "    classifier_yaml = classifier.to_yaml()\n",
    "    classifier_name = model_params[\"model_name\"]\n",
    "    with open(f\"{classifier_name}.yaml\", \"w\") as f:\n",
    "        f.write(classifier_yaml)\n",
    "        classifier.save(f\"{classifier_name}.h5\")\n",
    "        print(\"Saving model to disk as a file\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating  function to predict on the test data\n",
    "def model_predict(classifier_name):\n",
    "    # Load model and its weighs from disk\n",
    "    loaded_file = open(f\"{classifier_name}.yaml\", \"r\")\n",
    "    loaded_model = loaded_file.read()\n",
    "    loaded_file.close()\n",
    "    classifier_model = model_from_yaml(loaded_model)\n",
    "\n",
    "    # Loading weights \n",
    "    load_model(f\"{classifier_name}.h5\")\n",
    "    print(\"Loaded model from disk\\n\\n\\n\")\n",
    "\n",
    "    # Test data path\n",
    "    img_dir = \"dataset_test/\" # Test directory path\n",
    "\n",
    "    # Iterate over each test image\n",
    "    data_path = os.path.join(img_dir, \"*g\")\n",
    "    files = glob.glob(data_path)\n",
    "    \n",
    "    # Make a prediction and add to results \n",
    "    data = []\n",
    "    results = []\n",
    "\n",
    "    for f1 in files:\n",
    "        img = image.load_img(f1, target_size = (64, 64))\n",
    "        img = image.img_to_array(img)\n",
    "        img = np.expand_dims(img, axis = 0)\n",
    "        data.append(img)\n",
    "        result = classifier_model.predict(img)\n",
    "        r = np.argmax(result, axis=1)\n",
    "        results.append(r)\n",
    "\n",
    "    return(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for the accuracy\n",
    "def accuracy_calc(predictions):\n",
    "    actual_values = [3, 0, 2, 0, 1, 2, 3, 1]\n",
    "    accuracy = metrics.accuracy_score(actual_values,predictions)\n",
    "    return(accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to run each model from the parameter grid\n",
    "def model_run(parameters):\n",
    "    model = parameters[\"model_name\"]\n",
    "    model_fit_save(parameters)\n",
    "    res = model_predict(model)\n",
    "    return(accuracy_calc(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model: {'model_name': 'model_1', 'steps_per_epoch': 1, 'epochs': 1}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.5319 - accuracy: 0.9375\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_2', 'steps_per_epoch': 1, 'epochs': 2}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4794 - accuracy: 0.9062\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.4075 - accuracy: 0.9062\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_3', 'steps_per_epoch': 1, 'epochs': 3}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3561 - accuracy: 0.8750\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.3272 - accuracy: 0.8750\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_4', 'steps_per_epoch': 2, 'epochs': 4}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/4\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.1552 - accuracy: 0.9688\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.1036 - accuracy: 0.9643\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 0s 135ms/step - loss: 0.1158 - accuracy: 0.9375\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 0s 130ms/step - loss: 0.2948 - accuracy: 0.9219\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_5', 'steps_per_epoch': 2, 'epochs': 5}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/5\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0988 - accuracy: 0.9643\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 0s 141ms/step - loss: 0.1625 - accuracy: 0.9688\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 0.1714 - accuracy: 0.9821\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.1811 - accuracy: 0.9643\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 0.0756 - accuracy: 0.9821\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_6', 'steps_per_epoch': 2, 'epochs': 6}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/6\n",
      "2/2 [==============================] - 0s 144ms/step - loss: 0.0672 - accuracy: 0.9821\n",
      "Epoch 2/6\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.0901 - accuracy: 0.9821\n",
      "Epoch 3/6\n",
      "2/2 [==============================] - 0s 132ms/step - loss: 0.0451 - accuracy: 1.0000\n",
      "Epoch 4/6\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 0.0635 - accuracy: 1.0000\n",
      "Epoch 5/6\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 0.0691 - accuracy: 0.9643\n",
      "Epoch 6/6\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 0.0471 - accuracy: 0.9821\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_7', 'steps_per_epoch': 3, 'epochs': 7}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/7\n",
      "3/3 [==============================] - 0s 152ms/step - loss: 0.0579 - accuracy: 0.9659\n",
      "Epoch 2/7\n",
      "3/3 [==============================] - 0s 158ms/step - loss: 0.0467 - accuracy: 0.9773\n",
      "Epoch 3/7\n",
      "3/3 [==============================] - 1s 172ms/step - loss: 0.0155 - accuracy: 1.0000\n",
      "Epoch 4/7\n",
      "3/3 [==============================] - 1s 183ms/step - loss: 0.0280 - accuracy: 0.9886\n",
      "Epoch 5/7\n",
      "3/3 [==============================] - 1s 178ms/step - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 6/7\n",
      "3/3 [==============================] - 1s 183ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 7/7\n",
      "3/3 [==============================] - 1s 199ms/step - loss: 0.0242 - accuracy: 0.9886\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_8', 'steps_per_epoch': 3, 'epochs': 8}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/8\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 2/8\n",
      "3/3 [==============================] - 1s 199ms/step - loss: 0.0246 - accuracy: 0.9886\n",
      "Epoch 3/8\n",
      "3/3 [==============================] - 1s 184ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 4/8\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 5/8\n",
      "3/3 [==============================] - 1s 176ms/step - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 6/8\n",
      "3/3 [==============================] - 0s 157ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 7/8\n",
      "3/3 [==============================] - 0s 164ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 8/8\n",
      "3/3 [==============================] - 1s 169ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_9', 'steps_per_epoch': 5, 'epochs': 9}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/9\n",
      "3/5 [=================>............] - ETA: 0s - loss: 0.0018 - accuracy: 1.0000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 45 batches). You may need to use the repeat() function when building your dataset.\n",
      "3/5 [=================>............] - 1s 182ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_10', 'steps_per_epoch': 5, 'epochs': 10}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/10\n",
      "3/5 [=================>............] - ETA: 0s - loss: 0.0024 - accuracy: 1.0000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
      "3/5 [=================>............] - 1s 177ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.125, 0.25, 0.5, 0.25, 0.125, 0.625, 0.125, 0.25, 0.5, 0.25]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import io\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "with ThreadPoolExecutor(1) as executor:\n",
    "    executor_results =  executor.map(model_run,parameter_grid)\n",
    "executor_results = list(executor_results)\n",
    "executor_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.125, 0.25, 0.5, 0.25, 0.125, 0.625, 0.125, 0.25, 0.5, 0.25]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executor_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>steps_per_epoch</th>\n",
       "      <th>epochs</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>model_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>model_2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>model_3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>model_4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>model_5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>model_6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>model_7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>model_8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>model_9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>model_10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name  steps_per_epoch  epochs  Accuracy\n",
       "0    model_1                1       1     0.125\n",
       "1    model_2                1       2     0.250\n",
       "2    model_3                1       3     0.500\n",
       "3    model_4                2       4     0.250\n",
       "4    model_5                2       5     0.125\n",
       "5    model_6                2       6     0.625\n",
       "6    model_7                3       7     0.125\n",
       "7    model_8                3       8     0.250\n",
       "8    model_9                5       9     0.500\n",
       "9   model_10                5      10     0.250"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets get the results \n",
    "final_results_df = pd.DataFrame(parameter_grid)\n",
    "final_results_df.insert(3, \"Accuracy\", executor_results, True) \n",
    "final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy results of all our 10 different models are extremely poor with just 1 ThreadPool Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the model: {'model_name': 'model_1', 'steps_per_epoch': 1, 'epochs': 1}\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_2', 'steps_per_epoch': 1, 'epochs': 2}\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_3', 'steps_per_epoch': 1, 'epochs': 3}\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_4', 'steps_per_epoch': 2, 'epochs': 4}\n",
      "\n",
      "\n",
      "Fitting the model: {'model_name': 'model_5', 'steps_per_epoch': 2, 'epochs': 5}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Found 88 images belonging to 4 classes.\n",
      "Found 88 images belonging to 4 classes.Found 88 images belonging to 4 classes.\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/5\n",
      "Epoch 1/3\n",
      "Epoch 1/4\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "2/2 [==============================] - 1s 699ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Epoch 2/2\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 1s 672ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0023 - accuracy: 2.0000\n",
      "Fitting the model: {'model_name': 'model_6', 'steps_per_epoch': 2, 'epochs': 6}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 2/4\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.0051 - accuracy: 1.00Epoch 3/3\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0051 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 2.2453e-04 - accuracy: 1.0000Epoch 1/6\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 4.3726e-04 - accuracy: 1.0000Fitting the model: {'model_name': 'model_7', 'steps_per_epoch': 3, 'epochs': 7}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "2/2 [==============================] - 1s 519ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 1s 389ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0051 - accuracy: 1.0000Saving model to disk as a file\n",
      "\n",
      "Epoch 3/4\n",
      "Epoch 1/7\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 7.1679e-04 - accuracy: 1.0000Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "2/2 [==============================] - 0s 246ms/step - loss: 5.7551e-04 - accuracy: 1.0000\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 0.0013 - accuracy: 1.0000Epoch 2/6\n",
      "Fitting the model: {'model_name': 'model_8', 'steps_per_epoch': 3, 'epochs': 8}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "2/2 [==============================] - 1s 507ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "2/2 [==============================] - 1s 260ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.0017 - accuracy: 1.0000    Epoch 4/4\n",
      "Epoch 4/5\n",
      "Epoch 1/8\n",
      "2/2 [==============================] - 0s 200ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "3/3 [==============================] - 1s 242ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 2/7\n",
      "Epoch 3/6\n",
      "2/2 [==============================] - 1s 439ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "2/2 [==============================] - 1s 571ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "2/2 [==============================] - 0s 168ms/step - loss: 8.9429e-04 - accuracy: 1.0000\n",
      "3/3 [==============================] - 1s 297ms/step - loss: 8.3938e-04 - accuracy: 1.0000\n",
      "3/3 [==============================] - ETA: 0s - loss: 7.7515e-04 - accuracy: 1.0000Saving model to disk as a file\n",
      "\n",
      "3/3 [==============================] - 1s 403ms/step - loss: 7.7515e-04 - accuracy: 1.0000\n",
      "\n",
      "Epoch 2/8\n",
      "Epoch 5/5\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Epoch 4/6\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 6.9230e-04 - accuracy: 1.0000Fitting the model: {'model_name': 'model_9', 'steps_per_epoch': 5, 'epochs': 9}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 5.4094e-04 - accuracy: 1.0000Epoch 1/9\n",
      "2/2 [==============================] - 0s 236ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.0034 - accuracy: 1.0000    Saving model to disk as a file\n",
      "\n",
      "2/2 [==============================] - 1s 315ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "3/3 [==============================] - 2s 560ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "3/3 [==============================] - 1s 444ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 4/7\n",
      "Epoch 3/8\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Epoch 5/6\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0120 - accuracy: 1.0000Fitting the model: {'model_name': 'model_10', 'steps_per_epoch': 5, 'epochs': 10}\n",
      "\n",
      "\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/3 [=========>....................] - ETA: 0s - loss: 0.0108 - accuracy: 1.0000Epoch 1/10\n",
      "1/2 [==============>...............] - ETA: 0s - loss: 9.7157e-04 - accuracy: 1.0000WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 45 batches). You may need to use the repeat() function when building your dataset.\n",
      "3/5 [=================>............] - 2s 675ms/step - loss: 9.2132e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "2/2 [==============================] - 0s 239ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "3/3 [==============================] - 1s 306ms/step - loss: 9.4508e-04 - accuracy: 1.0000\n",
      "3/3 [==============================] - 1s 430ms/step - loss: 9.7902e-04 - accuracy: 1.0000\n",
      "Epoch 4/8\n",
      "Epoch 5/7\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Epoch 6/6\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.0011 - accuracy: 1.0000    WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 50 batches). You may need to use the repeat() function when building your dataset.\n",
      "3/5 [=================>............] - 2s 581ms/step - loss: 9.1535e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "2/2 [==============================] - 0s 173ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "3/3 [==============================] - 1s 287ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 6/7\n",
      "3/3 [==============================] - 1s 424ms/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 5/8\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "3/3 [==============================] - 1s 255ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "2/3 [===================>..........] - ETA: 0s - loss: 0.0012 - accuracy: 1.0000Epoch 7/7\n",
      "3/3 [==============================] - 1s 202ms/step - loss: 3.4234e-04 - accuracy: 1.0000\n",
      "Epoch 6/8\n",
      "3/3 [==============================] - 1s 249ms/step - loss: 3.4296e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n",
      "3/3 [==============================] - 1s 219ms/step - loss: 4.1140e-04 - accuracy: 1.0000\n",
      "Epoch 7/8\n",
      "3/3 [==============================] - 1s 207ms/step - loss: 5.9787e-04 - accuracy: 1.0000\n",
      "Epoch 8/8\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 3.5484e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.25, 0.375, 0.25, 0.25, 0.25, 0.25, 0.375, 0.25, 0.25, 0.25]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets lets run our 10 models on 5 ThreadPool Executors\n",
    "with ThreadPoolExecutor(5) as executor:\n",
    "    executor_results =  executor.map(model_run,parameter_grid)\n",
    "executor_results = list(executor_results)\n",
    "executor_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>steps_per_epoch</th>\n",
       "      <th>epochs</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>model_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>model_2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>model_3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>model_4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>model_5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>model_6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>model_7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>model_8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>model_9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>model_10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name  steps_per_epoch  epochs  Accuracy\n",
       "0    model_1                1       1     0.250\n",
       "1    model_2                1       2     0.375\n",
       "2    model_3                1       3     0.250\n",
       "3    model_4                2       4     0.250\n",
       "4    model_5                2       5     0.250\n",
       "5    model_6                2       6     0.250\n",
       "6    model_7                3       7     0.375\n",
       "7    model_8                3       8     0.250\n",
       "8    model_9                5       9     0.250\n",
       "9   model_10                5      10     0.250"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets get the results \n",
    "final_results_df = pd.DataFrame(parameter_grid)\n",
    "final_results_df.insert(3, \"Accuracy\", executor_results, True) \n",
    "final_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again our results are poor even with 5 different ThreadPool Executors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try a different approach instead of running all 10 image classification models in parallel. We will run all 10 different individually and separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing a function to fit all 10 different models\n",
    "def fit_model(mod, steps, epo):\n",
    "    mod.fit(train_datagen.flow_from_directory('dataset_train', target_size=(64, 64), batch_size = 32,\n",
    "                    class_mode = 'categorical', seed = 69), steps_per_epoch=steps, epochs=epo)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function to save the model onto the disk\n",
    "\n",
    "def model_disk_save(m):\n",
    "    m_yaml = m.to_yaml()\n",
    "    m_name = m\n",
    "    with open(f\"{m}.yaml\", \"w\") as f:\n",
    "        f.write(m_yaml)\n",
    "        m.save(f\"{m}.h5\")\n",
    "        print(\"Saving model to disk as a file\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.1173e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = fit_model(classifier, 1, 1)\n",
    "model_disk_save(model_1)\n",
    "model_1_pred = model_predict(model_1)\n",
    "accuracy_model_1 = accuracy_calc(model_1_pred)\n",
    "accuracy_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/2\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5453e-04 - accuracy: 1.0000\n",
      "Epoch 2/2\n",
      "1/1 [==============================] - 0s 813us/step - loss: 1.4549e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = fit_model(classifier, 1, 2)\n",
    "model_disk_save(model_2)\n",
    "model_2_pred = model_predict(model_2)\n",
    "accuracy_model_2 = accuracy_calc(model_2_pred)\n",
    "accuracy_model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8435e-04 - accuracy: 1.0000\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 0s 878us/step - loss: 5.3897e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3 = fit_model(classifier, 1, 3)\n",
    "model_disk_save(model_3)\n",
    "model_3_pred = model_predict(model_1)\n",
    "accuracy_model_3 = accuracy_calc(model_3_pred)\n",
    "accuracy_model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/4\n",
      "2/2 [==============================] - 0s 102ms/step - loss: 2.1439e-04 - accuracy: 1.0000\n",
      "Epoch 2/4\n",
      "2/2 [==============================] - 0s 98ms/step - loss: 1.9956e-04 - accuracy: 1.0000\n",
      "Epoch 3/4\n",
      "2/2 [==============================] - 0s 100ms/step - loss: 1.2906e-04 - accuracy: 1.0000\n",
      "Epoch 4/4\n",
      "2/2 [==============================] - 0s 99ms/step - loss: 5.0428e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_4 = fit_model(classifier, 2, 4)\n",
    "model_disk_save(model_4)\n",
    "model_4_pred = model_predict(model_4)\n",
    "accuracy_model_4 = accuracy_calc(model_4_pred)\n",
    "accuracy_model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/5\n",
      "2/2 [==============================] - 0s 105ms/step - loss: 1.6831e-04 - accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 0s 136ms/step - loss: 3.0468e-04 - accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 4.3742e-04 - accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 0s 96ms/step - loss: 2.1983e-04 - accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 0s 95ms/step - loss: 1.5012e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5 = fit_model(classifier, 2, 5)\n",
    "model_disk_save(model_5)\n",
    "model_5_pred = model_predict(model_5)\n",
    "accuracy_model_5 = accuracy_calc(model_5_pred)\n",
    "accuracy_model_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/6\n",
      "2/2 [==============================] - 0s 134ms/step - loss: 9.4847e-04 - accuracy: 1.0000\n",
      "Epoch 2/6\n",
      "2/2 [==============================] - 0s 104ms/step - loss: 1.9333e-04 - accuracy: 1.0000\n",
      "Epoch 3/6\n",
      "2/2 [==============================] - 0s 101ms/step - loss: 3.0439e-04 - accuracy: 1.0000\n",
      "Epoch 4/6\n",
      "2/2 [==============================] - 0s 110ms/step - loss: 6.6808e-04 - accuracy: 1.0000\n",
      "Epoch 5/6\n",
      "2/2 [==============================] - 0s 142ms/step - loss: 6.3826e-04 - accuracy: 1.0000\n",
      "Epoch 6/6\n",
      "2/2 [==============================] - 0s 139ms/step - loss: 3.1712e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_6 = fit_model(classifier, 2, 6)\n",
    "model_disk_save(model_6)\n",
    "model_6_pred = model_predict(model_6)\n",
    "accuracy_model_6 = accuracy_calc(model_6_pred)\n",
    "accuracy_model_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/7\n",
      "3/3 [==============================] - 1s 185ms/step - loss: 2.0413e-04 - accuracy: 1.0000\n",
      "Epoch 2/7\n",
      "3/3 [==============================] - 1s 186ms/step - loss: 3.2823e-04 - accuracy: 1.0000\n",
      "Epoch 3/7\n",
      "3/3 [==============================] - 0s 145ms/step - loss: 2.2351e-04 - accuracy: 1.0000\n",
      "Epoch 4/7\n",
      "3/3 [==============================] - 1s 175ms/step - loss: 1.5286e-04 - accuracy: 1.0000\n",
      "Epoch 5/7\n",
      "3/3 [==============================] - 0s 150ms/step - loss: 2.4591e-04 - accuracy: 1.0000\n",
      "Epoch 6/7\n",
      "3/3 [==============================] - 1s 175ms/step - loss: 2.1253e-04 - accuracy: 1.0000\n",
      "Epoch 7/7\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 1.2280e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_7 = fit_model(classifier, 3, 7)\n",
    "model_disk_save(model_7)\n",
    "model_7_pred = model_predict(model_7)\n",
    "accuracy_model_7 = accuracy_calc(model_7_pred)\n",
    "accuracy_model_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 images belonging to 4 classes.\n",
      "Epoch 1/8\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 2.3734e-04 - accuracy: 1.0000\n",
      "Epoch 2/8\n",
      "3/3 [==============================] - 0s 153ms/step - loss: 1.5050e-04 - accuracy: 1.0000\n",
      "Epoch 3/8\n",
      "3/3 [==============================] - 0s 149ms/step - loss: 2.3864e-04 - accuracy: 1.0000\n",
      "Epoch 4/8\n",
      "3/3 [==============================] - 0s 151ms/step - loss: 1.7684e-04 - accuracy: 1.0000\n",
      "Epoch 5/8\n",
      "3/3 [==============================] - 0s 155ms/step - loss: 6.9642e-05 - accuracy: 1.0000\n",
      "Epoch 6/8\n",
      "3/3 [==============================] - 0s 154ms/step - loss: 1.4227e-04 - accuracy: 1.0000\n",
      "Epoch 7/8\n",
      "3/3 [==============================] - 1s 177ms/step - loss: 1.0295e-04 - accuracy: 1.0000\n",
      "Epoch 8/8\n",
      "3/3 [==============================] - 0s 147ms/step - loss: 2.4272e-04 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.375"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_8 = fit_model(classifier, 3, 8)\n",
    "model_disk_save(model_8)\n",
    "model_8_pred = model_predict(model_8)\n",
    "accuracy_model_8 = accuracy_calc(model_8_pred)\n",
    "accuracy_model_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7017e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.2064e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.5868e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.6082e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.5361e-05 - accuracy: 1.0000\n",
      "Epoch 1\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4637e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.1461e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0753e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.8978e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.3109e-05 - accuracy: 1.0000\n",
      "Epoch 2\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2425e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0908e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6055e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.3190e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 988us/step - loss: 8.0351e-05 - accuracy: 1.0000\n",
      "Epoch 3\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.0399e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.0411e-04 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1926e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.8358e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.7537e-05 - accuracy: 1.0000\n",
      "Epoch 4\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.8589e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.9665e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8388e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.4234e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.4839e-05 - accuracy: 1.0000\n",
      "Epoch 5\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.6936e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.5656e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 999us/step - loss: 3.5381e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0656e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.2281e-05 - accuracy: 1.0000\n",
      "Epoch 6\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.5450e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 9.2026e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.2835e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 5.7540e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 971us/step - loss: 6.9895e-05 - accuracy: 1.0000\n",
      "Epoch 7\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4106e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.8742e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 968us/step - loss: 3.0651e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4729e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.7699e-05 - accuracy: 1.0000\n",
      "Epoch 8\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2862e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.5745e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.8784e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 979us/step - loss: 5.2194e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.5674e-05 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_9 \n",
    "\n",
    "n_batch = 5\n",
    "\n",
    "n_epochs=9\n",
    "\n",
    "# loop through epochs\n",
    "for e in range(n_epochs):\n",
    "    print('Epoch', e)\n",
    "    batches = 0\n",
    "      # loop through batches\n",
    "    for x_batch, y_batch in train_datagen.flow_from_directory('dataset_train/', target_size=(64, 64), batch_size = 32, class_mode = 'categorical', seed = 74):\n",
    "        classifier.fit(x_batch, y_batch)\n",
    "        batches += 1\n",
    "        if batches >= n_batch:\n",
    "        # we need to break the loop by hand because the generator loops indefinitely\n",
    "            break\n",
    "            \n",
    "model_9 = classifier\n",
    "model_disk_save(model_9)\n",
    "model_9_pred = model_predict(model_9)\n",
    "accuracy_model_9 = accuracy_calc(model_9_pred)\n",
    "accuracy_model_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.1700e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.2938e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.7161e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.9874e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.3816e-05 - accuracy: 1.0000\n",
      "Epoch 1\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.0616e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 8.0346e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.5722e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.7756e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.2099e-05 - accuracy: 1.0000\n",
      "Epoch 2\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.9618e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.7926e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.4431e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.5820e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.0506e-05 - accuracy: 1.0000\n",
      "Epoch 3\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8639e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.5736e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 981us/step - loss: 2.3279e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.4103e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.8968e-05 - accuracy: 1.0000\n",
      "Epoch 4\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7734e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.3729e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.2237e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 4.2539e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.7524e-05 - accuracy: 1.0000\n",
      "Epoch 5\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6870e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.1853e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 2.1284e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 997us/step - loss: 4.1087e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.6191e-05 - accuracy: 1.0000\n",
      "Epoch 6\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6032e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 7.0051e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 2.0395e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 941us/step - loss: 3.9717e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.4928e-05 - accuracy: 1.0000\n",
      "Epoch 7\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.5246e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.8326e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.9591e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.8458e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.3767e-05 - accuracy: 1.0000\n",
      "Epoch 8\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.4490e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.6688e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 1.8841e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.7274e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.2665e-05 - accuracy: 1.0000\n",
      "Epoch 9\n",
      "Found 88 images belonging to 4 classes.\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.3764e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 6.5098e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 1.8151e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 3.6131e-05 - accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 5.1618e-05 - accuracy: 1.0000\n",
      "Saving model to disk as a file\n",
      "\n",
      "Loaded model from disk\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_10\n",
    "\n",
    "n_batch = 5\n",
    "\n",
    "n_epochs=10\n",
    "\n",
    "# loop through epochs\n",
    "for e in range(n_epochs):\n",
    "    print('Epoch', e)\n",
    "    batches = 0\n",
    "      # loop through batches\n",
    "    for x_batch, y_batch in train_datagen.flow_from_directory('dataset_train/', target_size=(64, 64), batch_size = 32, class_mode = 'categorical', seed = 74):\n",
    "        classifier.fit(x_batch, y_batch)\n",
    "        batches += 1\n",
    "        if batches >= n_batch:\n",
    "        # we need to break the loop by hand because the generator loops indefinitely\n",
    "            break\n",
    "            \n",
    "model_10 = classifier\n",
    "model_disk_save(model_10)\n",
    "model_10_pred = model_predict(model_10)\n",
    "accuracy_model_10 = accuracy_calc(model_10_pred)\n",
    "accuracy_model_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_name</th>\n",
       "      <th>steps_per_epoch</th>\n",
       "      <th>epochs</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>model_1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>model_2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>model_3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>model_4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>model_5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>model_6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>model_7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>model_8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0.375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>model_9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>model_10</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_name  steps_per_epoch  epochs  Accuracy\n",
       "0    model_1                1       1     0.250\n",
       "1    model_2                1       2     0.375\n",
       "2    model_3                1       3     0.250\n",
       "3    model_4                2       4     0.250\n",
       "4    model_5                2       5     0.250\n",
       "5    model_6                2       6     0.250\n",
       "6    model_7                3       7     0.250\n",
       "7    model_8                3       8     0.375\n",
       "8    model_9                5       9     0.125\n",
       "9   model_10                5      10     0.250"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now lets compare the final results\n",
    "\n",
    "compare_results_df = pd.DataFrame(parameter_grid)\n",
    "Accuracy_list = [accuracy_model_1,accuracy_model_2,accuracy_model_3,accuracy_model_4,accuracy_model_5,accuracy_model_6,\n",
    "    accuracy_model_7,accuracy_model_8,accuracy_model_9,accuracy_model_10]\n",
    "compare_results_df.insert(3, \"Accuracy\", Accuracy_list, True) \n",
    "compare_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like our results turned out to be same as the ThreadPool executor approach that leveraged parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Questions: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Discuss the effect of the following on accuracy and loss (train & test): \n",
    "\n",
    "    Increasing the steps_per_epoch\n",
    "    Increasing the number of epochs\n",
    "\n",
    "Increasing the steps_per_epoch : Steps per epoch refers to the magnitude or the number of iterations of batches before a single epoch of the training process ends or gets completed. steps_per_epoch is all about updating the weights in each iteration through the process of gradient descent. Computation is increased to a great extent wity the increase in the steps_per_epoch parameter value because it involves the update on the weight values and it also leads to the increase in time it takes to complete each epoch . The loss function value is minimized to a great extent as we impose more updates on the weight values involving more number of iterations. The lowering of the loss function ultimately leads to the increase in the accuracy of the predictions which decreases the bias of the model.As we know considering the bias-variance tradeoff that increase in accuracy/decrease in the bias comes with the consequence or expense of increase in the variance of the model and potentially causing the overfitting of the model which then needs to the generalized through dropout mechanisms if we use neural nets.\n",
    "\n",
    "\n",
    "Increasing the number of epochs :  If we take a particular neural network into consideration, an epoch in such a case is considered to be completed or finished when the entirety of the input training dataset is passed both backward and forward through the entire neural network just once. During such each process, an epoch refers to a hyperparameter that indicates how many times all of the training vectors are used once in order to update the weights of the nodes in the neural network or the internal model parameters. Epoch indicates the number of compelete passes through the entire training dataset. Accuracy of the model predictions increase with the increase in the number of epochs which in turn leads to the decrease in the model errors or the bias. However for the validation set, a lowest point of global minima is reached for the model errors beyond which the error values increases again . This usually happens when the predictive model starts to overfit and incorporates the noise instead of the general patterns within the training data. So this overfit cannot perform accurately when it is exposed to the new unseen data. Early stopping is a very effective strategy or technique to stop this overfitting where the validation set error value reaches the global minimum so that we dont have to over-train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Name two uses of zero padding in CNN.\n",
    "\n",
    "Zero padding is a technique that is widely used in the image classification which involves surrounding a particular matrix of a convolutional neural network with a bunch of zeroes. This technique helps in preserving the image features that exist at the edges of the original matrix which in turn helps in controlling the size of the output feature map. Padding in general works by extending the area that actually processes the image in the receptive field of the convolutional neural network. Some of the advantages of the zero padding are as follows :\n",
    "\n",
    "Zero padding helps us to preserve the original input size.Zero padding involves adding a border or layer of pixels with zero value around the edges of the input training/test images. Zero padding helps in retaining more important information at the borders more accurately for analysis. In convolutional neural networks in particular , a kernel is a filter within neural networks which slides or moves across the entire image being part of the receptive field, thereby scanning the every pixel of the image and then converts the pixel data into smaller or larger format. The zero padding helps in assisting the kernel in the processing of the image which involves the padding being added to the frame of the image in order to facilitate or allow the kernel to cover the image for processing. It is very likely and easy for the pixels in the corners of the input image to get ignored by the kernel and features or the pixels at the center would dominate the output or those center pixels could repeatedly contribute to the output feature map. The pixels on the border could easily be washed away or ignored very quickly as the volume size reduces after each convolutional layer by a small amount when we dont stack a few layers of zero padding in the convolutional neural network. A much better performance in terms of model accuracy could be obtained by our convolutional neural network by including the technique of zero padding which makes sure that the border pixels of the input image are not lost.\n",
    "\n",
    "Zero padding helps in building deeper networks that process the input images on a much deeper level.Spatial size of the output feature map could also be dramaticaly controlled using zero padding. The spatial size of the output volume is controlled by or dominated by a few factors which include the size of the size of the receptive field, the amount of zero padding , volume size of the input, the stride ( distance between two consecutive receptive fields) . The size of the spatial output volume is directly proportional to the amount of zero padding layers which helps massively in reducing the output volume size too quickly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. What is the use of a 1 x 1 kernel in CNN? \n",
    "\n",
    "\n",
    "One of the major use of the 1 x 1 kernel within the realm of the convlutioinal neural networks is the dimensionality reduction or augmentation by reducing the paremeter map. We all know by now that a convolutional layer with a 1 x 1 filter can be helpful in CNN to have control over the number of input feature maps. A simple 1 x 1 filter can be used to summarize the input feature maps. When we stack these 1 x 1 filters and make use of multiple 1 x 1 filters , the tuning of the number of sumaries of the input feature maps can be accomplished which in turn significantly allows the depth of the feature maps to be increased or decreased as needed.\n",
    "\n",
    "A 1 x 1 kernel helps in mappping the pixels in the input to the output pixel to the next layers with all its channels. These type of simple 1 x 1 kernels are computationally least expensive and are also efficient in their operational functionality.\n",
    "\n",
    "It also helps in building a deeper network. An effective down-sampling could happen in a 1 x 1 kernel leading to smaller feature vectors  or reduced amount of feature parameters.Identity shortcuts can be directly used as the 1 x 1 kernel restores the original dimensions to match the dimension of the input.\n",
    "\n",
    "\n",
    "Non-linearity could be added additionally to the neural network using these 1 x 1 kernels.\n",
    "\n",
    "\n",
    "A 1 x 1 kernel could also increase the accuracy to a great extent by creating a smaller convolutional neural network.In most of the case for the CNNs to have higher accuracy , they are required to have huge number of complex parameters which cause a massive increase in the training time and requires large amount of computational resources.To overcome these kind of drawbacks , its better to have a simple 1 x 1 kernel which include a squeeze layer and an expansion layer to come up with the same level of accuracy as other complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. What are the advantages of a CNN over a fully connected DNN for this image classification problem?\n",
    "\n",
    "\n",
    "Since CNN is location invariant , it can really help with the aigility in pattern recognition.The CNN can recognize the pattern of an image or video in any other location once it has recognized the pattern in one location. This is the main dominating property of CNN called the feature extraction. On the other hand , when a DNN recognizes a pattern in one location , it still remains limited in its functiionality and can recognize that kind of a pattern only in that particular location.\n",
    "\n",
    "Weight sharing is another main advantage of using a CNN which reduces the number of parameters leading to increased efficiency.The same parameters are shared between all the neurans within a CNN thereby leading to the reduction of additional parameters that are needed by the CNN model which makes it a lot more flexible compared to the DNN model.\n",
    "As a consequence , CNNs also leverage the advantage of using local spatial coherence in the images from the input which allows the to have fewer weights because some of the parameter weights are shared. This is what enables them to make heavy computations at a low cost and they are well suited to extract any information in the form of text , images , voice , video etc\n",
    "\n",
    "CNNs also have pooling layers which helps a lot in down scaling the image. This happens because the information is retained throughout the network once its extracted and then features are organized spatially identically to the actual image leading to the downscaling of the image by actually reducing the size of the image.\n",
    "\n",
    "There is no sense of order of the inputs in traditional feedforward neural networks.Shuffling of the images in multiple ways does not impact the performance of the feedforward neural network.Where as CNN taking advantage of local spatial coherence , they can dramatically reduce the computation required in terms of operations needed to process the image by imposing convolutions on the patches of adjacent pixels because adjacent pixels are meaningful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
