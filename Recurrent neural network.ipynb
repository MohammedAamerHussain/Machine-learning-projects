{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    " # a) Import the following libraries:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import sys\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import LSTM,Dense,Dropout\n",
    "from keras.models import Sequential,load_model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from collections import OrderedDict\n",
    "import optparse\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "import json\n",
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) We will read the code in slightly differently than before:\n",
    "df = pd.read_csv(\"dev-access.csv\", engine='python', quotechar='|', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['{\"timestamp\":1502738402847,\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"source\":{\"remoteAddress\":\"88.141.113.237\",\"referer\":\"http://localhost:8002/enter\"},\"route\":\"/login\",\"headers\":{\"host\":\"localhost:8002\",\"accept-language\":\"en-us\",\"accept-encoding\":\"gzip, deflate\",\"connection\":\"keep-alive\",\"accept\":\"*/*\",\"referer\":\"http://localhost:8002/enter\",\"cache-control\":\"no-cache\",\"x-requested-with\":\"XMLHttpRequest\",\"content-type\":\"application/json\",\"content-length\":\"36\"},\"requestPayload\":{\"username\":\"Carl2\",\"password\":\"bo\"},\"responsePayload\":{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Invalid Login\"}}',\n",
       "       0], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # c) We then need to convert to a numpy.ndarray type:\n",
    "df = df.values\n",
    "df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26773, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " # d) Check the shape of the data set - it should be (26773, 2). Spend some time looking at the data.\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['{\"timestamp\":1502738402847,\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"source\":{\"remoteAddress\":\"88.141.113.237\",\"referer\":\"http://localhost:8002/enter\"},\"route\":\"/login\",\"headers\":{\"host\":\"localhost:8002\",\"accept-language\":\"en-us\",\"accept-encoding\":\"gzip, deflate\",\"connection\":\"keep-alive\",\"accept\":\"*/*\",\"referer\":\"http://localhost:8002/enter\",\"cache-control\":\"no-cache\",\"x-requested-with\":\"XMLHttpRequest\",\"content-type\":\"application/json\",\"content-length\":\"36\"},\"requestPayload\":{\"username\":\"Carl2\",\"password\":\"bo\"},\"responsePayload\":{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Invalid Login\"}}',\n",
       "        0],\n",
       "       ['{\"timestamp\":1502738402849,\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"source\":{\"remoteAddress\":\"88.141.113.237\"},\"route\":\"/login\",\"headers\":{\"host\":\"localhost:8002\",\"connection\":\"keep-alive\",\"cache-control\":\"no-cache\",\"accept\":\"*/*\",\"accept-encoding\":\"gzip, deflate, br\",\"accept-language\":\"en-US,en;q=0.8,es;q=0.6\",\"content-type\":\"application/json\",\"content-length\":\"47\"},\"requestPayload\":{\"username\":\"pafzah\",\"password\":\"worldburn432\"},\"responsePayload\":{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Invalid Login\"}}',\n",
       "        0],\n",
       "       ['{\"timestamp\":1502738402852,\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"source\":{\"remoteAddress\":\"205.49.83.118\"},\"route\":\"/login\",\"headers\":{\"host\":\"localhost:8002\",\"connection\":\"keep-alive\",\"cache-control\":\"no-cache\",\"accept\":\"*/*\",\"accept-encoding\":\"gzip, deflate, br\",\"accept-language\":\"en-US,en;q=0.8,es;q=0.6\",\"content-type\":\"application/json\",\"content-length\":\"44\"},\"requestPayload\":{\"username\":\"Panos1\",\"password\":\"najrijkom\"},\"responsePayload\":{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Invalid Login\"}}',\n",
       "        0],\n",
       "       ['{\"timestamp\":1502738402852,\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"source\":{\"remoteAddress\":\"205.49.83.118\",\"referer\":\"http://localhost:8002/enter\"},\"route\":\"/login\",\"headers\":{\"host\":\"localhost:8002\",\"accept-language\":\"en-us\",\"accept-encoding\":\"gzip, deflate\",\"connection\":\"keep-alive\",\"accept\":\"*/*\",\"referer\":\"http://localhost:8002/enter\",\"cache-control\":\"no-cache\",\"x-requested-with\":\"XMLHttpRequest\",\"content-type\":\"application/json\",\"content-length\":\"47\"},\"requestPayload\":{\"username\":\"vuvpuvehu\",\"password\":\"password1\"},\"responsePayload\":{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Invalid Login\"}}',\n",
       "        0],\n",
       "       ['{\"timestamp\":1502738402853,\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"source\":{\"remoteAddress\":\"137.196.95.116\"},\"route\":\"/login\",\"headers\":{\"host\":\"localhost:8002\",\"connection\":\"keep-alive\",\"cache-control\":\"no-cache\",\"accept\":\"*/*\",\"accept-encoding\":\"gzip, deflate, br\",\"accept-language\":\"en-US,en;q=0.8,es;q=0.6\",\"content-type\":\"application/json\",\"content-length\":\"41\"},\"requestPayload\":{\"username\":\"Michele\",\"password\":\"mokgu\"},\"responsePayload\":{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Invalid Login\"}}',\n",
       "        0],\n",
       "       ['{\"timestamp\":1502738402853,\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"source\":{\"remoteAddress\":\"137.196.95.116\"},\"route\":\"/login\",\"headers\":{\"host\":\"localhost:8002\",\"connection\":\"keep-alive\",\"cache-control\":\"no-cache\",\"accept\":\"*/*\",\"accept-encoding\":\"gzip, deflate, br\",\"accept-language\":\"en-US,en;q=0.8,es;q=0.6\",\"content-type\":\"application/json\",\"content-length\":\"38\"},\"requestPayload\":{\"username\":\"lem\",\"password\":\"Mc2402\"},\"responsePayload\":{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Invalid Login\"}}',\n",
       "        0]], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['{\"timestamp\":1502738402847,\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"source\":{\"remoteAddress\":\"88.141.113.237\",\"referer\":\"http://localhost:8002/enter\"},\"route\":\"/login\",\"headers\":{\"host\":\"localhost:8002\",\"accept-language\":\"en-us\",\"accept-encoding\":\"gzip, deflate\",\"connection\":\"keep-alive\",\"accept\":\"*/*\",\"referer\":\"http://localhost:8002/enter\",\"cache-control\":\"no-cache\",\"x-requested-with\":\"XMLHttpRequest\",\"content-type\":\"application/json\",\"content-length\":\"36\"},\"requestPayload\":{\"username\":\"Carl2\",\"password\":\"bo\"},\"responsePayload\":{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Invalid Login\"}}',\n",
       "       '{\"timestamp\":1502738402849,\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"source\":{\"remoteAddress\":\"88.141.113.237\"},\"route\":\"/login\",\"headers\":{\"host\":\"localhost:8002\",\"connection\":\"keep-alive\",\"cache-control\":\"no-cache\",\"accept\":\"*/*\",\"accept-encoding\":\"gzip, deflate, br\",\"accept-language\":\"en-US,en;q=0.8,es;q=0.6\",\"content-type\":\"application/json\",\"content-length\":\"47\"},\"requestPayload\":{\"username\":\"pafzah\",\"password\":\"worldburn432\"},\"responsePayload\":{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Invalid Login\"}}',\n",
       "       '{\"timestamp\":1502738402852,\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"source\":{\"remoteAddress\":\"205.49.83.118\"},\"route\":\"/login\",\"headers\":{\"host\":\"localhost:8002\",\"connection\":\"keep-alive\",\"cache-control\":\"no-cache\",\"accept\":\"*/*\",\"accept-encoding\":\"gzip, deflate, br\",\"accept-language\":\"en-US,en;q=0.8,es;q=0.6\",\"content-type\":\"application/json\",\"content-length\":\"44\"},\"requestPayload\":{\"username\":\"Panos1\",\"password\":\"najrijkom\"},\"responsePayload\":{\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Invalid Login\"}}',\n",
       "       ...,\n",
       "       '{\"timestamp\":1502738800390,\"method\":\"post\",\"query\":{},\"path\":\"/checkout\",\"statusCode\":400,\"source\":{\"remoteAddress\":\"233.150.201.166\"},\"route\":\"/checkout\",\"headers\":{\"host\":\"localhost:8002\",\"connection\":\"keep-alive\",\"cache-control\":\"no-cache\",\"accept\":\"*/*\",\"accept-encoding\":\"gzip, deflate, br\",\"accept-language\":\"en-US,en;q=0.8,es;q=0.6\",\"content-type\":\"application/json\",\"content-length\":\"80\"},\"requestPayload\":{\"creditCard\":\"<script src=\\\\\"http://attacker/maliciousâ€‘script.js\\\\\"></script>\"},\"responsePayload\":{\"statusCode\":400,\"error\":\"Bad Request\",\"message\":\"Bad Request\"}}',\n",
       "       '{\"timestamp\":1502738803362,\"method\":\"post\",\"query\":{},\"path\":\"/checkout\",\"statusCode\":400,\"source\":{\"remoteAddress\":\"182.239.131.78\",\"referer\":\"http://localhost:8002/enter\"},\"route\":\"/checkout\",\"headers\":{\"host\":\"localhost:8002\",\"connection\":\"keep-alive\",\"accept\":\"*/*\",\"cache-control\":\"no-cache\",\"x-requested-with\":\"XMLHttpRequest\",\"referer\":\"http://localhost:8002/enter\",\"accept-encoding\":\"gzip, deflate, br\",\"accept-language\":\"en-US,en;q=0.8,es;q=0.6\",\"content-type\":\"application/json\",\"content-length\":\"46\"},\"requestPayload\":{\"creditCard\":\"<meta http-equiv=\\\\\"refresh\\\\\">\"},\"responsePayload\":{\"statusCode\":400,\"error\":\"Bad Request\",\"message\":\"Bad Request\"}}',\n",
       "       '{\"timestamp\":1502738810082,\"method\":\"post\",\"query\":{},\"path\":\"/checkout\",\"statusCode\":400,\"source\":{\"remoteAddress\":\"182.239.131.78\"},\"route\":\"/checkout\",\"headers\":{\"host\":\"localhost:8002\",\"connection\":\"keep-alive\",\"cache-control\":\"no-cache\",\"accept\":\"*/*\",\"accept-encoding\":\"gzip, deflate, br\",\"accept-language\":\"en-US,en;q=0.8,es;q=0.6\",\"content-type\":\"application/json\",\"content-length\":\"46\"},\"requestPayload\":{\"creditCard\":\"<meta http-equiv=\\\\\"refresh\\\\\">\"},\"responsePayload\":{\"statusCode\":400,\"error\":\"Bad Request\",\"message\":\"Bad Request\"}}'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# e) Store all rows and the 0th index as the feature data:\n",
    "X = df[:,0]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 1, 1, 1], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f) Store all rows and index 1 as the target variable:\n",
    "Y = df[:,1]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g) In the next step, we will clean up the predictors. This includes removing features that are not valuable,\n",
    "#such as timestamp and source.\n",
    "\n",
    "for index, item in enumerate(X):\n",
    "    # Quick hack to space out json elements\n",
    "    reqJson = json.loads(item, object_pairs_hook=OrderedDict)\n",
    "    del reqJson['timestamp']\n",
    "    del reqJson['headers']\n",
    "    del reqJson['source']\n",
    "    del reqJson['route']\n",
    "    del reqJson['responsePayload']\n",
    "    X[index] = json.dumps(reqJson, separators=(',', ':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"method\":\"post\",\"query\":{},\"path\":\"/login\",\"statusCode\":401,\"requestPayload\":{\"username\":\"Carl2\",\"password\":\"bo\"}}'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h) We next will tokenize our data, which just means vectorizing our text. Given the data we will tokenize every character \n",
    "# (thus char_level = True)\n",
    "\n",
    "tokenizer = Tokenizer(filters='\\t\\n', char_level=True)\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# we will need this later\n",
    "num_words = len(tokenizer.word_index) + 1\n",
    "X = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\"': 1,\n",
       " 'e': 2,\n",
       " 't': 3,\n",
       " ':': 4,\n",
       " 'a': 5,\n",
       " 's': 6,\n",
       " 'o': 7,\n",
       " 'u': 8,\n",
       " 'r': 9,\n",
       " ',': 10,\n",
       " 'd': 11,\n",
       " 'l': 12,\n",
       " 'p': 13,\n",
       " 'h': 14,\n",
       " 'y': 15,\n",
       " 'q': 16,\n",
       " 'c': 17,\n",
       " '{': 18,\n",
       " '}': 19,\n",
       " 'm': 20}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(list(tokenizer.word_index.items())[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 1, 20, 2, 3, 14, 7, 11, 1, 4, 1, 13, 7, 6, 3, 1, 10, 1, 16, 8]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18 means '{', 1 means '\"' as definied byt the word index. \n",
    "So this is just the first row but as numbers now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18,\n",
       " 1,\n",
       " 20,\n",
       " 2,\n",
       " 3,\n",
       " 14,\n",
       " 7,\n",
       " 11,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 13,\n",
       " 7,\n",
       " 6,\n",
       " 3,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 16,\n",
       " 8,\n",
       " 2,\n",
       " 9,\n",
       " 15,\n",
       " 1,\n",
       " 4,\n",
       " 18,\n",
       " 19,\n",
       " 10,\n",
       " 1,\n",
       " 13,\n",
       " 5,\n",
       " 3,\n",
       " 14,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 25,\n",
       " 12,\n",
       " 7,\n",
       " 26,\n",
       " 24,\n",
       " 21,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 6,\n",
       " 3,\n",
       " 5,\n",
       " 3,\n",
       " 8,\n",
       " 6,\n",
       " 17,\n",
       " 7,\n",
       " 11,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 23,\n",
       " 22,\n",
       " 29,\n",
       " 10,\n",
       " 1,\n",
       " 9,\n",
       " 2,\n",
       " 16,\n",
       " 8,\n",
       " 2,\n",
       " 6,\n",
       " 3,\n",
       " 13,\n",
       " 5,\n",
       " 15,\n",
       " 12,\n",
       " 7,\n",
       " 5,\n",
       " 11,\n",
       " 1,\n",
       " 4,\n",
       " 18,\n",
       " 1,\n",
       " 8,\n",
       " 6,\n",
       " 2,\n",
       " 9,\n",
       " 21,\n",
       " 5,\n",
       " 20,\n",
       " 2,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 17,\n",
       " 5,\n",
       " 9,\n",
       " 12,\n",
       " 28,\n",
       " 1,\n",
       " 10,\n",
       " 1,\n",
       " 13,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 32,\n",
       " 7,\n",
       " 9,\n",
       " 11,\n",
       " 1,\n",
       " 4,\n",
       " 1,\n",
       " 40,\n",
       " 7,\n",
       " 1,\n",
       " 19,\n",
       " 19]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i) Need to pad our data as each observation has a different length\n",
    "max_log_length = 1024\n",
    "X_processed = sequence.pad_sequences(X, maxlen=max_log_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  1, 19, 19],\n",
       "       [ 0,  0,  0, ...,  1, 19, 19],\n",
       "       [ 0,  0,  0, ...,  1, 19, 19],\n",
       "       ...,\n",
       "       [ 0,  0,  0, ...,  1, 19, 19],\n",
       "       [ 0,  0,  0, ...,  1, 19, 19],\n",
       "       [ 0,  0,  0, ...,  1, 19, 19]], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 28, 29, 32, 40],\n",
       "       dtype=int32),\n",
       " array([909,  22,   7,   6,   7,   7,   7,   7,   4,   5,   5,   4,   3,\n",
       "          4,   2,   2,   2,   2,   3,   3,   2,   2,   1,   1,   1,   1,\n",
       "          1,   1,   1,   1,   1]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(X_processed[0], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j) Create your train set to be 75% of the data and your test set to be 25%\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, Y, test_size=.25,random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model l: Base RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) Start by creating an instance of a Sequential model:\n",
    "model_1 = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) From there, add an Embedding layer: https://keras.io/layers/embeddings/\n",
    "\n",
    "# Params:\n",
    "\n",
    "# input_dim = num_words (the variable we created above)\n",
    "# output_dim = 32\n",
    "# input_length = max_log_length (we also created this above)\n",
    "# Keep all other variables as the defaults (shown below)\n",
    "\n",
    "\n",
    "model_1.add(Embedding(input_dim = num_words, output_dim = 32, input_length = max_log_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c) Add a SimpleRNN layer:\n",
    "# Params:\n",
    "# units = 32\n",
    "# activation = 'relu'\n",
    "\n",
    "model_1.add(SimpleRNN(units = 32, activation = \"relu\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d) Finally, we will add a Dense layer:\n",
    "# Params:\n",
    "# units = 1 (this will be our output)\n",
    "# activation --> you can choose to use either relu or sigmoid. \n",
    "\n",
    "model_1.add(Dense(units=1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e) Compile model using the .compile() method:\n",
    "# Params:\n",
    "# loss = binary_crossentropy\n",
    "# optimizer = adam\n",
    "# metrics = accuracy\n",
    "model_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "simple_rnn (SimpleRNN)       (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,129\n",
      "Trainable params: 4,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(y_train).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.asarray(y_test).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "118/118 [==============================] - 27s 232ms/step - loss: 0.5366 - accuracy: 0.7024 - val_loss: 0.2446 - val_accuracy: 0.9323\n",
      "Epoch 2/3\n",
      "118/118 [==============================] - 28s 236ms/step - loss: 0.1171 - accuracy: 0.9683 - val_loss: 0.0408 - val_accuracy: 0.9896\n",
      "Epoch 3/3\n",
      "118/118 [==============================] - 27s 232ms/step - loss: 0.1036 - accuracy: 0.9702 - val_loss: 0.0579 - val_accuracy: 0.9865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fefe26e5450>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# g) Use the .fit() method to fit the model on the train data. Use validation_split=0.25, epochs=3 batch_size=128.\n",
    "\n",
    "model_1.fit(X_train, y_train,  epochs = 3, validation_split = 0.25, batch_size = 128, verbose =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 2s 47ms/step - loss: 0.0666 - accuracy: 0.9840\n",
      "Model 1 Test loss: 0.06656002998352051\n",
      "Model 1 Test accuracy: 0.9840155243873596\n"
     ]
    }
   ],
   "source": [
    "model_1_metrics = model_1.evaluate(X_test, y_test, batch_size = 128)\n",
    "print('Model 1 Test loss:', model_1_metrics[0])\n",
    "print('Model 1 Test accuracy:', model_1_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Model 2 - LSTM + Dropout Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) This RNN needs to have the following layers (add in this order):\n",
    "Embedding Layer (use same params as before)\n",
    "LSTM Layer (units = 64, recurrent_dropout = 0.5)\n",
    "Dropout Layer - use a value of 0.5\n",
    "Dense Layer - (use same params as before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Embedding layer\n",
    "model_2.add(Embedding(input_dim = num_words, output_dim = 32, input_length = max_log_length))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM layer\n",
    "model_2.add(LSTM(units = 64, recurrent_dropout = 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout layer for regularization\n",
    "model_2.add(Dropout(rate = 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer\n",
    "model_2.add(Dense(units = 1, activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) Compile model using the .compile() method:\n",
    "# Params:\n",
    "# loss = binary_crossentropy\n",
    "# optimizer = adam\n",
    "# metrics = accuracy\n",
    "\n",
    "model_2.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 64)                24832     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 26,913\n",
      "Trainable params: 26,913\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "118/118 [==============================] - 117s 988ms/step - loss: 0.4275 - accuracy: 0.7989 - val_loss: 0.1502 - val_accuracy: 0.9651\n",
      "Epoch 2/3\n",
      "118/118 [==============================] - 118s 1s/step - loss: 0.1233 - accuracy: 0.9649 - val_loss: 0.0859 - val_accuracy: 0.9813\n",
      "Epoch 3/3\n",
      "118/118 [==============================] - 116s 984ms/step - loss: 0.1105 - accuracy: 0.9727 - val_loss: 0.1056 - val_accuracy: 0.9769\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fefc4f73c10>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(X_train, y_train, epochs = 3, validation_split = 0.25, batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 7s 126ms/step - loss: 0.1002 - accuracy: 0.9773\n",
      "Model 2 Test loss: 0.10024955868721008\n",
      "Model 2 Test accuracy: 0.977293074131012\n"
     ]
    }
   ],
   "source": [
    "# e) Use the .evaluate() method to get the loss value & the accuracy value on the test data. Use a batch size of\n",
    "# 128 again.\n",
    "model_2_metrics = model_2.evaluate(X_test, y_test, batch_size = 128)\n",
    "print('Model 2 Test loss:', model_2_metrics[0])\n",
    "print('Model 2 Test accuracy:', model_2_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Recurrent Neural Net Model 3: Build Your Own"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You wil now create your RNN based on what you have learned from Model 1 & Model 2:\n",
    "a) RNN Requirements:\n",
    "Use 5 or more layers\n",
    "Add a layer that was not utilized in Model 1 or Model 2 (Note: This could be a new Dense layer or an\n",
    "additional LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 1024, 32)          2016      \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 1024, 64)          24832     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1024, 64)          0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 59,937\n",
      "Trainable params: 59,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_3 = Sequential()\n",
    "model_3.add(Embedding(num_words, 32, input_length=max_log_length))\n",
    "model_3.add(LSTM(64,recurrent_dropout=0.5,return_sequences= True))\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(LSTM(64, recurrent_dropout=0.5))\n",
    "model_3.add(Dropout(0.5))\n",
    "model_3.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model_3.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "118/118 [==============================] - 239s 2s/step - loss: 0.4805 - accuracy: 0.7824 - val_loss: 0.1283 - val_accuracy: 0.9691\n",
      "Epoch 2/3\n",
      "118/118 [==============================] - 240s 2s/step - loss: 0.2085 - accuracy: 0.9385 - val_loss: 0.0829 - val_accuracy: 0.9773\n",
      "Epoch 3/3\n",
      "118/118 [==============================] - 239s 2s/step - loss: 0.1393 - accuracy: 0.9593 - val_loss: 0.0705 - val_accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fef55113950>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(X_train, y_train, epochs=3, batch_size=128, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 14s 258ms/step - loss: 0.0727 - accuracy: 0.9797\n",
      "Model 3 Test loss: 0.07269810140132904\n",
      "Model 3 Test accuracy: 0.9796832799911499\n"
     ]
    }
   ],
   "source": [
    "model_3_metrics = model_3.evaluate(X_test, y_test, batch_size = 128)\n",
    "print('Model 3 Test loss:', model_3_metrics[0])\n",
    "print('Model 3 Test accuracy:', model_3_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conceptual Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Explain the difference between the relu activation function and the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function in the deep learning realm decides what is to be fired to the next neuron within th network or whether a neuron gets activated or not. This mechanism plays a crucial role for the deep learning network in learning complex patterns in the data that as fed to it. The relu and the sigmoid activation functions are one of the most widely used activation functions for the deep learning.\n",
    "\n",
    "The sigmoid fucntion is one of the most popular activation functions that is widely used in binary classification problems and its output lies between 0 and 1. It is curvy which is makes it differential at any point between this range . The sigmoid function causes the input values that range between -2 and 2 to be very sensitive to change and if the input values are outside of this range then this sigmoid function becomes saturated between the output values of 0 and 1. this bounded nature of the sigmoid function creates a limitation and a weakness where the error gradients drop close to zero called as vanishing gradient problem where the model stagnates and doesnt learn or improve in performance.As we all know that a typical deep learning  neura network uses backpropagation mechanism in order to calculate the error gradients and the perform an update on the weights. If the output of the sigmoid function turns out to be very small,then this could lead to the vanishing gradient problem where the weights of the lower layer  weights do not unchanged and the training never really results or converges to a good solution.That is the reason  ReLU is prefered to sigmoid.\n",
    "\n",
    "ReLU(REctified linear unit) is an activation function that never really becomes flat because it never saturates as it output values range from 0 to infinity. This is one of the reasons why the gradient descent really works well for ReLU. The ReLU function gives 0 output value if the input is non-positive and it gives positive values if the input is positive. ReLU is considered to be a piecewise function because half of its output is linear because of positive output and the rest/other half is non-linear. The ReLU fucntion helps in preserving the compelx properties of the input data because linear part of this function makes the linear models easy to optimize using gradient descent with no vanishing gradient issue/problem. But it has been noticed that the grdient breaks  with zero or negative value inputs. That is why it is recommended to start with very small but positive weights for this reason. The ReLU function is fast and its efficient because it doesnt activate all of the neurons within the network and that is one of the main advantages of using ReLU.  The ReLu is perfectly capable of giving an output of a true 0 value  in contrast to the sigmoid function that only learns to approximate a 0 output (i.e. a value very close to 0 but not an actual true 0). This is largely driven by how the ReLu oprrates or functions and allowance of such true 0\n",
    "values means certain neurons will not get activated leading to a sparse representation of a matrix or a sparse matrix. This happens when ReLu converts negative values to 0 and this is a desirable property because it accelerates the process of learning,simplies the model and causes the reduction in the computation making ReLu less\n",
    "computationally expensive or taxing compared to that of the sigmoid function. Even an activation fucntion like ReLu has a limitation and a drawback where gradient is 0 and the neurons become stuck/inactive/dead neurons\n",
    "as this is caused by certain neurons that stop responding to variations in the input/error. This particular problem is called as the dying ReLu problem which can solved by using a variant of the ReLu function such as the LeaklyReLu which allows a small, positive gradient when the unit is not active, or a smaller learning rate.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Describe what one epoch actually is (epoch was a parameter used in the .fit() method)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a particular neural network into consideration, an epoch is considered to be completed or finished when the entirety of the input training dataset is passed both backward and forward through the entire neural network just once. During such each process, an epoch refers to a hyperparameter that indicates how many times all of the training vectors are used once in order to update the weights of the nodes in the neural network or the internal model parameters. Epoch indicates the number of compelete passes through the entire training dataset.\n",
    "\n",
    "we divide the entire training input datadset into multiple smaller batches as processing the entire dataset in just 1 epoch is too big to be fed into the model. That is why one more parameter into consideration called the batch_size. A single epoch is said to be complete when all of the batches within the dataset have been trained.\n",
    "Lets take an example where say we have a data of 5000 datapoints/rows. We cam divide the entire data into batches of 1000. In this case 1000 will be the batch_size and to complete 1 epoch, it will take 5 iterations. The weights of the\n",
    "neurons are updated during each iteration. The performance of our deep learning model/network  can only improve when we pass the entire training dataset multiple times to the same deep neural network and compute the error gradients through backpropagation because just passing the entire dataset through a neural network once (i.e. 1 epoch) is not enough.At the same time it is also important to make sure that we do not go through too many training epochs iterations as that might be counter-productive and also  that would lead to the model learning all the unnecssary noise within the training data and overfit.\n",
    "\n",
    "Accuracy of the model predictions increase with the increase in the number of epochs which in turn leads to the decrease in the model errors or the bias. However for the validation set, a lowest point of global minima is reached for the model errors beyond which the error values increases again . This usually happens when the predictive model starts to overfit and incorporates the noise instead of the general patterns within the training data. So this overfit cannot perform accurately when it is exposed to the new unseen data. Early stopping is a very effective strategy or technique to stop this overfitting where the validation set error value reaches the global minimum so that we dont have to over-train our model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Explain how dropout works (you can look at the keras code and/or documentation) for (a) training, and (b) test data sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a well known regularization technique in the deep learning world which helps in preventing overfitting by  dropping nodes randomly within the neural network between each training iteration. During each training iteration or epoch , certain neurons are selected randomly that are ignored or 'dropped out' and play no role in the calculation of error gradients which leads to a better generalization error. This is because the neural network will not be sensitive to weights of specific neurons or let certain neuron dominate the process of the calculation of the error gradients. The dropout is a  hyperparameter 'p' that controls the dropout rate. In this particular assignment we have set it to 50% or 0.5 for the models 2 and 3. This means that any particular node within the neural network has a 50/50 chance of being included in any given training iteration. This technique helps the neural network to adapt to dead/inactive/missing nodes and it helps the model to prevent incorporating the unnecssary noise within the model or  prevent the training data from being memorized.\n",
    "\n",
    "Dropout regularization process happens only during training step . At each step of training process, any neuron has a probability p (also called the dropout rate) of being entirely ignored or temporarily dropped out during this training step, but it is very likely that it may be active during the next training step . After training process has completed, the neurons will not be dropped anymore. So if we look from a broader perspective a unique neural network is created/generated at every training epoch/step. The resulting final neural network will be an average ensemble of all these smaller neural networks because we have a different neural network altogether at each training step, . This is what makes the dropout mechanism an effective and popular regularization technique.\n",
    "\n",
    "dropout applies only to training data and not test data.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Explain why problems such as this homework assignment are better modeled with RNNs than CNNs. What type of problem will CNNs outperform RNNs on?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This home work assignment dataset involves time series data and requires sequence modeling to be done on top of it. \n",
    "The data is in the form of a log file. Recurrent neural networks are typically applied to these kind of problems where a sequence of multiple steps taken as input is mapped to a class prediction( binary or 0 or 1). In other words , it is a many-to-one sequence prediction.\n",
    "\n",
    "The main differentiating factor for the RNNs is that the output of each layer becomes the input for itself( same layer) in the next training iteration. For this particular reason , RNNs are usually prefered for modelling timeseries data or sequences in the input data where the previous values or events(in this case) in that sequence matter a lot for making prediction( binary class prediction in this case). For this specific assignment, the order in which the sequence of events have taken place  helps to find/spot security breach or hacking and not just the metadata. This is what makes RNN is best suited for detection of security breaches.\n",
    "\n",
    "Convolutional neural networks that are the traditional feed-forward neural networks do\n",
    "not share imnformation about the features that have been processed across multiple different positions of the neural network. CNN like models assume independence of outputs from the inputs which makes them incapable of performing well for sequence prediction tasks. In our assignment, since the previous sequences in the inputs are inherently important in predicting the next output, RNN is much more suitable or the best fit for sequence modeling because of many reasons . One of the main reason is that they have memory that can be used or can serve as feedback loops .This looping mechanism then acts as a highway to allow information to flow from one training step to the next training step. This information is stored in the hidden state of neurons within the network, which represents previous inputs. Besides that, since RNNs are able to maintain state between sequence elements ,they are typically used with problems of this sequence modeling nature where mapping takes place between input sequence to fixed-sized output vectors. RNNs can handle are many-to-many , one-to-many, many-to-one types of sequence modeling, including spoken language represented a time series or just raw sequences of text(NLP) . while CNN strictly requires fixed length input , RNNs  can work on or be fed data of different variational lengths. Our assignment had time series data with varying lengths.\n",
    "\n",
    "Because of CNN's feature extraction property , they can easily outperform RNNs on image processing problems.\n",
    "CNNs develop an internal representation of an image. This is what amkes them particularly suitable for data with spatial relationship or images in general. Once the CNN model learns to recognize a pattern in one location, it can recognize it in any other location. CNNs can scale in variant structures in the input data and its pattern recognition capability is location invariant.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Explain what RNN problem is solved using LSTM and briefly describe how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs require a lot of training which certainly can take up a very long time because it will take hundreds of iterations in order for the model to learn long-term patterns  . The problem of long-term dependencies is one of the inherent problems of the Recurrent neural networks (RNN). The idea of connecting previous information to present tasks or in ther words sequential information was one of the main reasons for the creation of the RNNs. In order to to make a reasonable prediction of the present tasks , there are obviously situations where more context is needed.\n",
    "The main reason for poor performance by RNN is the gap between relevant information that is needed to make accurate predictions on the sequence modelling data and the present task at hand. So for fixing this particular issue , the most common technique is to simply cap the input sequence. This can be done  by either looking at recent data for timeseries modelling or by looking at a fixed number of inputs.  The problem with this specific approach is that a lot of information is lost during this process . So to encounter this problem a way is needed to include both long-term information  and also the most recent information at each and every training step. The most popular way of solving this problem of long-term dependencies is using Long Short Term Memory networks (LSTM),which is a special kind of RNN. \n",
    "\n",
    "The vanishing gradient problem in RNN is caused by its short term memory and this problem can be solved using LSTM .\n",
    "As RNN goes through many iteration and training steps to process more information,  it has trouble retaining information from previous steps. The nature of back-propagation is the main cause of this issue because its an algorithm that is used in the  optimization of the neural networks and also to train the NN. The main concept behind Back-propagation is that it uses the computed error gradients to update weights of the nodes/neurons within the neural network. The main reason for the short-term memory of the RNNs is because the gradient values will exponentially shrink as it propagates through each training step, leading in a minor adjustment to the weights of the  neural network  and also the early/initial layers contribute very less to the learning process. since the initial layers do not learn much , is it very easy for the neural network to forget what it's seen in longer sequences.\n",
    "\n",
    "\n",
    "The \"gate\" mechanism that is used to regulate the flow of information is mainly responsible to make LSTM capable of learning long-term dependencies . In order to learn what information to remove or add to the hidden state of neurons,\n",
    "thse kind of gates use different tensor operations .The short-term memory is less of an issue for the LSTM because of this specific ability. \n",
    "\n",
    "\n",
    "LSTM has two different memory inputs, one for short-term and the other one for long-term . The neural network learns to recognize an important sequence in the input and then store it in the long-term cell using these LSTM cells. Over many training iterations , it can continue to use that short term memory in the input  until it is no longer useful and the its gets dropped from the memory. This is the main reason that makes the LSTM cell very efficient for RNNs while training over long sequences of input data.\n",
    "\n",
    "\n",
    "The core concept behind the fucntioning of the LSTM networks are its gates  and the state of the LSTM cells. The transfer of the relative information down the sequence chain is handled by the individual cell states which act as a transporter highways. The crucial information from the earlier training sequence iterations or steps can make its way to later time steps to retrieved and used/processed .This happens because of the cell states that are very good in carrying the information that is relevant throughout the processing of the training sequence thereby reducing the short-term memory effects.The crucial information gets removed or added during this process, via these gates to the cell state . These LSTM neural networks can then learn what information is really relevant for the modelling and also which information to forget or which information to keep during training period via the sigmoid activation function using these LSTM gates which are separate neural networks altogether within themselves and they determine which information is allowed on the cell state.\n",
    "\n",
    "LSTMs networks expand the default chain structure within RNNs to add multiple four additional layers in each chain.\n",
    "The ability to remove or add information with the help of these gates in the LSTM network makes these layers complex comapred to a simple RNN. These gates include a pointwise multiplication operation and a sigmoid neural network layer. The sigmoid layer outputs a 1 or 10(1 allows ; 0 disallows ).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
